#!/usr/bin/env python3
"""
æ ‡å‡†åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
æ”¯æŒå…¨é¢çš„LLMåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…æ‹¬ MMLU, BBH, GPQA, MATH ç­‰30+ä¸»æµæµ‹è¯•é›†
"""

import subprocess
import json
import os
import time
from typing import Dict, List, Optional
from pathlib import Path

# å¯¼å…¥comprehensive benchmark configurations
from evaluations.standard_benchmarks.comprehensive_benchmark_config import (
    COMPREHENSIVE_BENCHMARKS, 
    BENCHMARK_SUITES,
    get_benchmark_by_category,
    get_benchmark_by_priority, 
    estimate_total_time,
    list_all_categories,
    get_suite_info
)

class StandardBenchmarkRunner:
    """æ ‡å‡†åŸºå‡†æµ‹è¯•è¿è¡Œå™¨ - æ”¯æŒ30+ä¸»æµåŸºå‡†æµ‹è¯•"""
    
    def __init__(self, model_url: str = "http://localhost:8000", model_name: str = "Qwen/Qwen3-32B"):
        self.model_url = model_url
        self.model_name = model_name
        self.results_dir = Path("evaluations/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨comprehensive benchmark configurations
        self.benchmarks = COMPREHENSIVE_BENCHMARKS
        self.benchmark_suites = BENCHMARK_SUITES
        
        print(f"ğŸ¯ å·²åŠ è½½ {len(self.benchmarks)} ä¸ªåŸºå‡†æµ‹è¯•é…ç½®")
        print(f"ğŸ“‹ æ”¯æŒ {len(self.benchmark_suites)} ä¸ªæµ‹è¯•å¥—ä»¶: {list(self.benchmark_suites.keys())}")
    
    def get_available_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨æµ‹è¯•ç±»åˆ«"""
        return list_all_categories()
    
    def get_benchmarks_by_category(self, category: str) -> List[str]:
        """æ ¹æ®ç±»åˆ«è·å–åŸºå‡†æµ‹è¯•åˆ—è¡¨"""
        return get_benchmark_by_category(category)
    
    def get_benchmarks_by_priority(self, priority: int) -> List[str]:
        """æ ¹æ®ä¼˜å…ˆçº§è·å–åŸºå‡†æµ‹è¯•åˆ—è¡¨"""
        return get_benchmark_by_priority(priority)
    
    def get_suite_benchmarks(self, suite_name: str) -> List[str]:
        """è·å–æµ‹è¯•å¥—ä»¶ä¸­çš„åŸºå‡†æµ‹è¯•åˆ—è¡¨"""
        return self.benchmark_suites.get(suite_name, [])
    
    def run_benchmark(self, benchmark_name: str, limit_samples: Optional[int] = None) -> Dict:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
        if benchmark_name not in self.benchmarks:
            raise ValueError(f"æœªçŸ¥çš„åŸºå‡†æµ‹è¯•: {benchmark_name}. å¯ç”¨çš„æµ‹è¯•: {list(self.benchmarks.keys())}")
        
        config = self.benchmarks[benchmark_name]
        print(f"ğŸš€ è¿è¡Œ {config.name} åŸºå‡†æµ‹è¯•...")
        print(f"ğŸ“ æè¿°: {config.description}")
        print(f"ğŸ”¢ ä»»åŠ¡æ•°é‡: {len(config.tasks)}")
        
        # æ„å»ºlm_evalå‘½ä»¤
        tasks_str = ",".join(config.tasks)
        model_args = f"base_url={self.model_url}/v1,model={self.model_name}"
        
        output_path = self.results_dir / f"{benchmark_name}_{int(time.time())}.json"
        
        # ä½¿ç”¨local-chat-completionsåç«¯ï¼Œè¿™æ˜¯ä¸ºæœ¬åœ°OpenAI APIå…¼å®¹æœåŠ¡å™¨è®¾è®¡çš„
        cmd = [
            "lm_eval", 
            "--model", "local-chat-completions",
            "--model_args", model_args,
            "--tasks", tasks_str,
            "--num_fewshot", str(config.num_fewshot),
            "--batch_size", str(config.batch_size),
            "--output_path", str(output_path),
            "--write_out",
            "--apply_chat_template"
        ]
        
        if limit_samples:
            cmd.extend(["--limit", str(limit_samples)])
        
        print(f"ğŸ’» æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä¸ºlocal-chat-completionsæä¾›å¿…è¦çš„é…ç½®
            env = os.environ.copy()
            env['OPENAI_API_KEY'] = 'dummy-key'  # local-chat-completionséœ€è¦è¿™ä¸ªç¯å¢ƒå˜é‡ï¼Œä½†å€¼å¯ä»¥æ˜¯ä»»æ„çš„
            
            # è¿è¡Œlm_eval
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600, env=env)
            
            if result.returncode != 0:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.stderr}")
                return {
                    "benchmark": benchmark_name,
                    "status": "failed", 
                    "error": result.stderr,
                    "stdout": result.stdout
                }
            
            # è¯»å–ç»“æœæ–‡ä»¶
            if output_path.exists():
                with open(output_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                print(f"âœ… {config.name} æµ‹è¯•å®Œæˆ")
                self._print_results_summary(results, config.name)
                
                return {
                    "benchmark": benchmark_name,
                    "status": "success",
                    "config": config.__dict__,
                    "results": results,
                    "output_file": str(output_path)
                }
            else:
                return {
                    "benchmark": benchmark_name,
                    "status": "failed",
                    "error": "ç»“æœæ–‡ä»¶æœªç”Ÿæˆ",
                    "stdout": result.stdout
                }
                
        except subprocess.TimeoutExpired:
            return {
                "benchmark": benchmark_name,
                "status": "timeout",
                "error": "æµ‹è¯•è¶…æ—¶ (1å°æ—¶)"
            }
        except Exception as e:
            return {
                "benchmark": benchmark_name,
                "status": "error",
                "error": str(e)
            }
    
    def run_multiple_benchmarks(self, benchmark_names: List[str], limit_samples: Optional[int] = None) -> Dict:
        """è¿è¡Œå¤šä¸ªåŸºå‡†æµ‹è¯•"""
        print(f"ğŸ¯ å¼€å§‹è¿è¡Œ {len(benchmark_names)} ä¸ªåŸºå‡†æµ‹è¯•...")
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "model_url": self.model_url,
            "model_name": self.model_name,
            "limit_samples": limit_samples,
            "benchmarks": {}
        }
        
        for i, benchmark_name in enumerate(benchmark_names, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“Š æµ‹è¯•è¿›åº¦ {i}/{len(benchmark_names)}: {benchmark_name.upper()}")
            print("="*80)
            
            result = self.run_benchmark(benchmark_name, limit_samples)
            all_results["benchmarks"][benchmark_name] = result
            
            # æµ‹è¯•é—´éš”
            if i < len(benchmark_names):
                print("â³ ç­‰å¾…10ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(10)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = self.results_dir / f"benchmark_summary_{int(time.time())}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“„ æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
        
        # æ‰“å°æ€»ç»“
        self._print_overall_summary(all_results)
        
        return all_results
    
    def _print_results_summary(self, results: Dict, benchmark_name: str):
        """æ‰“å°å•ä¸ªæµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"\nğŸ“Š {benchmark_name} ç»“æœæ‘˜è¦:")
        print("-" * 50)
        
        if "results" in results:
            for task_name, task_result in results["results"].items():
                if isinstance(task_result, dict) and any(k.endswith("acc") or k.endswith("exact_match") for k in task_result.keys()):
                    # æ‰¾åˆ°å‡†ç¡®ç‡æŒ‡æ ‡
                    acc_keys = [k for k in task_result.keys() if k.endswith("acc") or k.endswith("exact_match") or k == "acc_norm"]
                    for acc_key in acc_keys:
                        score = task_result[acc_key]
                        if isinstance(score, (int, float)):
                            print(f"   â€¢ {task_name} ({acc_key}): {score:.1%}")
                        break
    
    def _print_overall_summary(self, all_results: Dict):
        """æ‰“å°æ€»ä½“æµ‹è¯•æ‘˜è¦"""
        print(f"\n{'='*80}")
        print("ğŸ† åŸºå‡†æµ‹è¯•æ€»ä½“ç»“æœ")
        print("="*80)
        
        for benchmark_name, result in all_results["benchmarks"].items():
            if result["status"] == "success" and "results" in result:
                config = self.benchmarks.get(benchmark_name, BenchmarkConfig(benchmark_name, []))
                print(f"\nğŸ“ˆ {config.name}:")
                
                # è®¡ç®—å¹³å‡å¾—åˆ†
                total_score = 0
                task_count = 0
                
                for task_name, task_result in result["results"]["results"].items():
                    if isinstance(task_result, dict):
                        acc_keys = [k for k in task_result.keys() if k.endswith("acc") or k.endswith("exact_match") or k == "acc_norm"]
                        for acc_key in acc_keys:
                            score = task_result[acc_key] 
                            if isinstance(score, (int, float)):
                                total_score += score
                                task_count += 1
                                break
                
                if task_count > 0:
                    avg_score = total_score / task_count
                    print(f"   å¹³å‡å¾—åˆ†: {avg_score:.1%}")
                else:
                    print("   çŠ¶æ€: å®Œæˆ")
            elif result["status"] == "failed":
                print(f"\nâŒ {benchmark_name}: æµ‹è¯•å¤±è´¥")
            elif result["status"] == "timeout":
                print(f"\nâ° {benchmark_name}: æµ‹è¯•è¶…æ—¶")

    def list_available_benchmarks(self, show_details: bool = False):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•"""
        print(f"ğŸ“‹ å¯ç”¨çš„åŸºå‡†æµ‹è¯• ({len(self.benchmarks)} ä¸ª):")
        print("="*80)
        
        if show_details:
            # æŒ‰ç±»åˆ«æ˜¾ç¤º
            categories = self.get_available_categories()
            for category in categories:
                benchmarks = self.get_benchmarks_by_category(category)
                print(f"\nğŸ“‚ {category.upper()} ({len(benchmarks)} ä¸ª):")
                for name in benchmarks:
                    config = self.benchmarks[name]
                    priority_emoji = "ğŸŒŸ" if config.priority == 1 else "âœ…" if config.priority == 2 else "ğŸ“‹"
                    print(f"   {priority_emoji} {name:<15} - {config.description}")
                    print(f"      ä»»åŠ¡æ•°: {len(config.tasks)}, Few-shot: {config.num_fewshot}, ä¼°è®¡æ—¶é—´: {config.estimated_time_minutes}min")
        else:
            # ç®€åŒ–æ˜¾ç¤º
            for name, config in self.benchmarks.items():
                priority_emoji = "ğŸŒŸ" if config.priority == 1 else "âœ…" if config.priority == 2 else "ğŸ“‹"
                print(f"{priority_emoji} {name:<15} - {config.description}")
        
        print("\nğŸ“¦ é¢„å®šä¹‰æµ‹è¯•å¥—ä»¶:")
        print("="*50)
        for suite_name, benchmarks in self.benchmark_suites.items():
            suite_info = get_suite_info(suite_name)
            time_str = f"{suite_info.get('estimated_time_hours', 0)}h" if 'estimated_time_hours' in suite_info else "æœªçŸ¥"
            print(f"â€¢ {suite_name:<15} - {len(benchmarks)}ä¸ªæµ‹è¯• (é¢„è®¡{time_str})")
            if show_details:
                print(f"   åŒ…å«: {', '.join(benchmarks[:5])}{', ...' if len(benchmarks) > 5 else ''}")
        
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("  --benchmarks mmlu,bbh            # è¿è¡ŒæŒ‡å®šæµ‹è¯•")  
        print("  --benchmarks all                 # è¿è¡Œæ‰€æœ‰æµ‹è¯•")
        print("  --benchmarks suite:core          # è¿è¡Œæ ¸å¿ƒæµ‹è¯•å¥—ä»¶")
        print("  --benchmarks category:reasoning  # è¿è¡Œæ¨ç†ç±»æµ‹è¯•")
        print("  --benchmarks priority:1          # è¿è¡Œä¼˜å…ˆçº§1çš„æµ‹è¯•")
        print("="*80)
    
    def resolve_benchmark_names(self, benchmarks_arg: str) -> List[str]:
        """è§£æåŸºå‡†æµ‹è¯•åç§°å‚æ•°ï¼Œæ”¯æŒå¥—ä»¶ã€ç±»åˆ«ã€ä¼˜å…ˆçº§"""
        if benchmarks_arg == "all":
            return list(self.benchmarks.keys())
        
        if benchmarks_arg.startswith("suite:"):
            suite_name = benchmarks_arg[6:]
            return self.get_suite_benchmarks(suite_name)
        
        if benchmarks_arg.startswith("category:"):
            category = benchmarks_arg[9:]
            return self.get_benchmarks_by_category(category)
        
        if benchmarks_arg.startswith("priority:"):
            priority = int(benchmarks_arg[9:])
            return self.get_benchmarks_by_priority(priority)
        
        # æ™®é€šçš„é€—å·åˆ†éš”åˆ—è¡¨
        return [b.strip() for b in benchmarks_arg.split(",")]