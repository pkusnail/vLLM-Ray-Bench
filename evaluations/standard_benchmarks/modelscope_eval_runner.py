#!/usr/bin/env python3
"""
ModelScope EvalScope é›†æˆè¿è¡Œå™¨
ä½¿ç”¨ModelScopeçš„EvalScopeæ¡†æ¶è¿›è¡Œæ ‡å‡†åŸºå‡†æµ‹è¯•
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from typing import List, Dict, Optional

# åŠ¨æ€è·å–é¡¹ç›®æ ¹è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
try:
    from evaluations.standard_benchmarks.comprehensive_benchmark_config import (
        COMPREHENSIVE_BENCHMARKS, 
        BENCHMARK_SUITES
    )
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
    from collections import namedtuple
    BenchmarkConfig = namedtuple('BenchmarkConfig', ['name', 'description', 'priority'])
    COMPREHENSIVE_BENCHMARKS = {
        'mmlu': BenchmarkConfig('MMLU', 'Massive Multitask Language Understanding', 1),
        'bbh': BenchmarkConfig('BBH', 'Big Bench Hard', 1),
        'gsm8k': BenchmarkConfig('GSM8K', 'å°å­¦æ•°å­¦åº”ç”¨é¢˜', 1),
        'math': BenchmarkConfig('MATH', 'é«˜éš¾åº¦æ•°å­¦é—®é¢˜æ±‚è§£', 1),
        'hellaswag': BenchmarkConfig('HellaSwag', 'å¸¸è¯†æ¨ç† - æƒ…å¢ƒç»­å†™', 2),
        'winogrande': BenchmarkConfig('WinoGrande', 'ä»£è¯æ¶ˆè§£æ¨ç†', 2),
        'cmmlu': BenchmarkConfig('CMMLU', 'Chinese MMLU', 2),
        'ceval': BenchmarkConfig('C-Eval', 'ä¸­æ–‡ç»¼åˆèƒ½åŠ›è¯„ä¼°', 2),
        'arc_challenge': BenchmarkConfig('ARC Challenge', 'ç§‘å­¦æ¨ç†æŒ‘æˆ˜é¢˜', 2),
        'arc_easy': BenchmarkConfig('ARC Easy', 'åŸºç¡€ç§‘å­¦æ¨ç†é¢˜', 2)
    }
    BENCHMARK_SUITES = {}

class ModelScopeEvalRunner:
    """ModelScope EvalScopeè¿è¡Œå™¨"""
    
    def __init__(self, api_url: str = "http://localhost:8000/v1/chat/completions", api_key: str = "dummy"):
        self.api_url = api_url
        self.api_key = api_key
        # ä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
        project_root = Path(__file__).parent.parent.parent
        self.results_dir = project_root / "evaluations" / "results" / "modelscope"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤ç”Ÿæˆé…ç½®
        self.default_generation_config = {
            "max_tokens": 2048,
            "temperature": 0.0,
            "top_p": 0.95
        }
        
        # ModelScope EvalScopeæ”¯æŒçš„åŸºå‡†æµ‹è¯•æ˜ å°„
        self.supported_benchmarks = {
            # æ•°å­¦æ¨ç†
            "gsm8k": "gsm8k",
            "math": "math", 
            
            # å¸¸è¯†æ¨ç†
            "hellaswag": "hellaswag",
            "winogrande": "winogrande",
            
            # å¤šå­¦ç§‘çŸ¥è¯†
            "mmlu": "mmlu",
            "cmmlu": "cmmlu",
            
            # é€»è¾‘æ¨ç†
            "bbh": "bbh", 
            
            # é˜…è¯»ç†è§£
            "arc_challenge": "arc_challenge",
            "arc_easy": "arc_easy",
            "arc": "arc_challenge",  # åˆ«å
            
            # ä¸­æ–‡è¯„ä¼°
            "ceval": "ceval",
            
            # å…¶ä»–åŸºå‡†æµ‹è¯•
            "piqa": "piqa"
        }
    
    def list_supported_benchmarks(self):
        """åˆ—å‡ºModelScope EvalScopeæ”¯æŒçš„åŸºå‡†æµ‹è¯•"""
        print("ğŸ“‹ ModelScope EvalScopeæ”¯æŒçš„åŸºå‡†æµ‹è¯•:")
        print("="*60)
        for key, modelscope_name in self.supported_benchmarks.items():
            if key in COMPREHENSIVE_BENCHMARKS:
                config = COMPREHENSIVE_BENCHMARKS[key]
                priority_emoji = "ğŸŒŸ" if config.priority == 1 else "âœ…" if config.priority == 2 else "ğŸ“‹"
                print(f"{priority_emoji} {key:<15} - {config.description}")
        print("="*60)
    
    def run_benchmark(self, benchmark_name: str, limit_samples: Optional[int] = None, 
                     generation_config: Optional[Dict] = None, seed: Optional[int] = None,
                     batch_size: int = 4) -> Dict:
        """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
        if benchmark_name not in self.supported_benchmarks:
            print(f"âŒ åŸºå‡†æµ‹è¯• {benchmark_name} ä¸è¢«ModelScope EvalScopeæ”¯æŒ")
            return {"status": "unsupported", "benchmark": benchmark_name}
        
        modelscope_name = self.supported_benchmarks[benchmark_name]
        config = COMPREHENSIVE_BENCHMARKS.get(benchmark_name)
        
        print(f"ğŸš€ ä½¿ç”¨ModelScope EvalScopeè¿è¡Œ {config.name if config else benchmark_name} åŸºå‡†æµ‹è¯•...")
        if config:
            print(f"ğŸ“ æè¿°: {config.description}")
        
        # åˆå¹¶ç”Ÿæˆé…ç½®
        final_generation_config = self.default_generation_config.copy()
        if generation_config:
            final_generation_config.update(generation_config)
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "evalscope", "eval",
            "--eval-type", "openai_api",
            "--api-url", self.api_url.replace('/v1/chat/completions', '/v1'),  # EvalScopeéœ€è¦base URL
            "--api-key", self.api_key,
            "--model", "Qwen/Qwen3-32B",  # æŒ‡å®šæ¨¡å‹åç§°
            "--datasets", modelscope_name,
            "--eval-batch-size", str(batch_size),
            "--generation-config", json.dumps(final_generation_config),
            "--work-dir", str(self.results_dir / f"{benchmark_name}_{int(time.time())}")
        ]
        
        # æ·»åŠ seedå‚æ•°
        if seed is not None:
            cmd.extend(["--seed", str(seed)])
        
        if limit_samples:
            cmd.extend(["--limit", str(limit_samples)])
        
        print(f"ğŸ’» æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡å’Œå·¥ä½œç›®å½•
            env = os.environ.copy()
            project_root = Path(__file__).parent.parent.parent
            
            # è¿è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd, 
                cwd=str(project_root),
                capture_output=True, 
                text=True, 
                timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
                env=env
            )
            
            if result.returncode != 0:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.stderr}")
                return {
                    "benchmark": benchmark_name,
                    "status": "failed",
                    "error": result.stderr,
                    "stdout": result.stdout
                }
            
            print(f"âœ… {benchmark_name} æµ‹è¯•å®Œæˆ")
            
            return {
                "benchmark": benchmark_name,
                "status": "success",
                "config": config.__dict__ if config else {},
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                "benchmark": benchmark_name,
                "status": "timeout",
                "error": "æµ‹è¯•è¶…æ—¶ (30åˆ†é’Ÿ)"
            }
        except Exception as e:
            return {
                "benchmark": benchmark_name,
                "status": "error",
                "error": str(e)
            }
    
    def run_multiple_benchmarks(self, benchmark_names: List[str], limit_samples: Optional[int] = None) -> Dict:
        """è¿è¡Œå¤šä¸ªåŸºå‡†æµ‹è¯•"""
        print(f"ğŸ¯ å¼€å§‹ä½¿ç”¨ModelScope EvalScopeè¿è¡Œ {len(benchmark_names)} ä¸ªåŸºå‡†æµ‹è¯•...")
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "api_url": self.api_url,
            "limit_samples": limit_samples,
            "framework": "ModelScope EvalScope",
            "benchmarks": {}
        }
        
        successful_tests = 0
        for i, benchmark_name in enumerate(benchmark_names, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“Š æµ‹è¯•è¿›åº¦ {i}/{len(benchmark_names)}: {benchmark_name.upper()}")
            print("="*80)
            
            result = self.run_benchmark(benchmark_name, limit_samples)
            all_results["benchmarks"][benchmark_name] = result
            
            if result["status"] == "success":
                successful_tests += 1
            
            # æµ‹è¯•é—´éš”
            if i < len(benchmark_names):
                print("â³ ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(5)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = self.results_dir / f"modelscope_summary_{int(time.time())}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ModelScope EvalScopeè¯„æµ‹å®Œæˆ!")
        print(f"ğŸ“„ æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
        print(f"ğŸ“Š æˆåŠŸæµ‹è¯•: {successful_tests}/{len(benchmark_names)}")
        
        return all_results
    
    def run_advanced_batch(self, benchmark_names: List[str], limit_samples: Optional[int] = None,
                          generation_config: Optional[Dict] = None, batch_size: int = 16, 
                          seed: int = 123) -> Dict:
        """è¿è¡Œé«˜çº§æ‰¹é‡æµ‹è¯•ï¼Œæ”¯æŒè‡ªå®šä¹‰é…ç½®"""
        print(f"ğŸš€ å¼€å§‹é«˜çº§æ‰¹é‡æµ‹è¯•æ¨¡å¼...")
        print(f"ğŸ“Š æµ‹è¯•åˆ—è¡¨: {benchmark_names}")
        print(f"ğŸ”§ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"ğŸ² éšæœºç§å­: {seed}")
        
        if generation_config:
            print(f"âš™ï¸  ç”Ÿæˆé…ç½®: {json.dumps(generation_config, indent=2)}")
        
        if limit_samples:
            print(f"ğŸ“ˆ æ ·æœ¬é™åˆ¶: {limit_samples}")
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "api_url": self.api_url,
            "limit_samples": limit_samples,
            "framework": "ModelScope EvalScope (Advanced)",
            "generation_config": generation_config or self.default_generation_config,
            "batch_size": batch_size,
            "seed": seed,
            "benchmarks": {}
        }
        
        successful_tests = 0
        for i, benchmark_name in enumerate(benchmark_names, 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“Š é«˜çº§æµ‹è¯•è¿›åº¦ {i}/{len(benchmark_names)}: {benchmark_name.upper()}")
            print("="*80)
            
            result = self.run_benchmark(
                benchmark_name=benchmark_name,
                limit_samples=limit_samples,
                generation_config=generation_config,
                seed=seed,
                batch_size=batch_size
            )
            all_results["benchmarks"][benchmark_name] = result
            
            if result["status"] == "success":
                successful_tests += 1
            
            # æµ‹è¯•é—´éš”
            if i < len(benchmark_names):
                print("â³ ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(5)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = self.results_dir / f"advanced_batch_{int(time.time())}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ é«˜çº§æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“„ æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
        print(f"ğŸ“Š æˆåŠŸæµ‹è¯•: {successful_tests}/{len(benchmark_names)}")
        
        return all_results

def main():
    """ä¸»å‡½æ•°ç”¨äºæµ‹è¯•"""
    runner = ModelScopeEvalRunner()
    
    print("ğŸ” æµ‹è¯•ModelScope EvalScopeä¸vLLMé›†ç¾¤çš„è¿æ¥...")
    
    # æµ‹è¯•æ”¯æŒçš„åŸºå‡†æµ‹è¯•åˆ—è¡¨
    runner.list_supported_benchmarks()
    
    # è¿è¡Œä¸€ä¸ªç®€å•æµ‹è¯•
    print("\nğŸ§ª è¿è¡ŒGSM8Kæ ‡å‡†æµ‹è¯•...")
    result = runner.run_benchmark("gsm8k", limit_samples=2)
    print(f"æ ‡å‡†æµ‹è¯•ç»“æœ: {result['status']}")
    
    # è¿è¡Œé«˜çº§æ‰¹é‡æµ‹è¯•ç¤ºä¾‹
    print("\nğŸš€ æµ‹è¯•é«˜çº§æ‰¹é‡åŠŸèƒ½...")
    advanced_config = {
        "max_tokens": 512,
        "temperature": 0.2,
        "top_p": 0.95
    }
    
    batch_result = runner.run_advanced_batch(
        benchmark_names=["gsm8k", "hellaswag"],
        limit_samples=2,
        generation_config=advanced_config,
        batch_size=8,
        seed=123
    )
    print(f"é«˜çº§æ‰¹é‡æµ‹è¯•ç»“æœ: å®Œæˆ {len([r for r in batch_result['benchmarks'].values() if r['status'] == 'success'])}/{len(batch_result['benchmarks'])} ä¸ªæµ‹è¯•")

if __name__ == "__main__":
    main()