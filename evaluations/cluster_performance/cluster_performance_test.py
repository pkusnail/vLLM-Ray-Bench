#!/usr/bin/env python3
"""
vLLMé›†ç¾¤æ€§èƒ½ä¸“é¡¹æµ‹è¯•
ä¸“æ³¨æµ‹è¯•é›†ç¾¤çš„ååé‡ã€å»¶è¿Ÿã€å¹¶å‘èƒ½åŠ›ç­‰ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼Œå¿½ç•¥å›ç­”å†…å®¹è´¨é‡
"""

import requests
import json
import time
import threading
import concurrent.futures
import statistics
from datetime import datetime

def simple_request(api_url, prompt, request_id):
    """ç®€å•è¯·æ±‚ï¼Œä¸“æ³¨æ€§èƒ½æµ‹è¯•"""
    payload = {
        "model": "Qwen/Qwen3-32B",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 50,  # å›ºå®šè¾ƒå°çš„tokenæ•°ï¼Œä¸“æ³¨æµ‹è¯•ç³»ç»Ÿæ€§èƒ½
        "temperature": 0.0  # å›ºå®šå‚æ•°ç¡®ä¿ä¸€è‡´æ€§
    }
    
    start_time = time.time()
    try:
        response = requests.post(api_url, json=payload)
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            response_time = end_time - start_time
            
            return {
                "request_id": request_id,
                "success": True,
                "response_time": response_time,
                "tokens": result['usage']['total_tokens'],
                "prompt_tokens": result['usage']['prompt_tokens'],
                "completion_tokens": result['usage']['completion_tokens'],
                "first_token_time": None  # ç®€åŒ–æµ‹è¯•ï¼Œä¸è®¡ç®—TTFT
            }
        else:
            return {
                "request_id": request_id,
                "success": False,
                "error": f"HTTP {response.status_code}",
                "response_time": end_time - start_time
            }
    except Exception as e:
        return {
            "request_id": request_id,
            "success": False,
            "error": str(e),
            "response_time": time.time() - start_time
        }

def run_performance_scenario(scenario_name, concurrent_requests, total_requests, prompt, api_url):
    """è¿è¡Œå•ä¸ªæ€§èƒ½æµ‹è¯•åœºæ™¯"""
    print(f"\nğŸš€ æ‰§è¡Œåœºæ™¯: {scenario_name}")
    print(f"   å¹¶å‘æ•°: {concurrent_requests}")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æµ‹è¯•æç¤º: {prompt[:50]}...")
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
        futures = []
        for i in range(total_requests):
            future = executor.submit(simple_request, api_url, prompt, i+1)
            futures.append(future)
        
        results = []
        completed = 0
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            results.append(result)
            completed += 1
            if completed % max(1, total_requests // 10) == 0:  # æ˜¾ç¤ºè¿›åº¦
                print(f"   è¿›åº¦: {completed}/{total_requests} ({completed/total_requests*100:.0f}%)")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # åˆ†æç»“æœ
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    if successful_results:
        response_times = [r['response_time'] for r in successful_results]
        total_tokens = sum(r['tokens'] for r in successful_results)
        
        metrics = {
            "scenario_name": scenario_name,
            "test_config": {
                "concurrent_requests": concurrent_requests,
                "total_requests": total_requests,
                "prompt": prompt
            },
            "results": {
                "total_time": total_time,
                "successful_requests": len(successful_results),
                "failed_requests": len(failed_results),
                "success_rate": len(successful_results) / total_requests * 100,
                
                # å»¶è¿ŸæŒ‡æ ‡
                "avg_response_time": statistics.mean(response_times),
                "median_response_time": statistics.median(response_times),
                "p95_response_time": statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times),
                "p99_response_time": statistics.quantiles(response_times, n=100)[98] if len(response_times) >= 100 else max(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                
                # ååé‡æŒ‡æ ‡  
                "requests_per_second": len(successful_results) / total_time,
                "tokens_per_second": total_tokens / total_time,
                "avg_tokens_per_request": total_tokens / len(successful_results),
                
                # èµ„æºæ•ˆç‡
                "total_tokens_processed": total_tokens,
                "cluster_efficiency": len(successful_results) / (concurrent_requests * total_time) # æ¯å¹¶å‘æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°
            },
            "raw_data": results
        }
        
        print(f"   âœ… å®Œæˆæ—¶é—´: {total_time:.2f}s")
        print(f"   âœ… æˆåŠŸç‡: {metrics['results']['success_rate']:.1f}%")
        print(f"   âš¡ å¹³å‡å»¶è¿Ÿ: {metrics['results']['avg_response_time']:.3f}s")
        print(f"   âš¡ P95å»¶è¿Ÿ: {metrics['results']['p95_response_time']:.3f}s")  
        print(f"   ğŸš€ è¯·æ±‚ååé‡: {metrics['results']['requests_per_second']:.2f} req/s")
        print(f"   ğŸš€ Tokenååé‡: {metrics['results']['tokens_per_second']:.1f} tok/s")
        
        return metrics
    else:
        print(f"   âŒ æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†!")
        return {
            "scenario_name": scenario_name,
            "error": "All requests failed",
            "results": results
        }

def test_cluster_performance():
    api_url = "http://localhost:8000/v1/chat/completions"
    
    print("=" * 80)
    print("âš¡ vLLMé›†ç¾¤æ€§èƒ½ä¸“é¡¹æµ‹è¯•")
    print("=" * 80)
    print("ğŸ¯ æµ‹è¯•ç›®æ ‡ï¼šè¯„ä¼°é›†ç¾¤çš„ååé‡ã€å»¶è¿Ÿã€å¹¶å‘å¤„ç†èƒ½åŠ›")
    print("ğŸ“Š æµ‹è¯•æ–¹å¼ï¼šå¤šç§è´Ÿè½½æ¨¡å¼ï¼Œå›ºå®šçŸ­å›ç­”ï¼Œä¸“æ³¨ç³»ç»Ÿæ€§èƒ½")
    print("ğŸ”§ æ¶æ„ï¼šPP=2, TP=8 (2 nodes, 16 GPUs)")
    print()
    
    # ä½¿ç”¨ç®€å•ç»Ÿä¸€çš„æµ‹è¯•æç¤ºï¼Œé¿å…å†…å®¹å¤æ‚åº¦å½±å“æ€§èƒ½æµ‹è¯•
    test_prompt = "What is the capital of China?"
    
    # æ€§èƒ½æµ‹è¯•åœºæ™¯çŸ©é˜µ
    performance_scenarios = [
        # åŸºçº¿å»¶è¿Ÿæµ‹è¯•
        {"name": "åŸºçº¿å»¶è¿Ÿ", "concurrent": 1, "total": 10, "description": "å•çº¿ç¨‹åŸºå‡†å»¶è¿Ÿ"},
        {"name": "ä½å¹¶å‘", "concurrent": 2, "total": 20, "description": "2å¹¶å‘ç¨³å®šæ€§"},
        {"name": "ä¸­å¹¶å‘", "concurrent": 5, "total": 50, "description": "5å¹¶å‘æ€§èƒ½"},
        {"name": "é«˜å¹¶å‘", "concurrent": 10, "total": 100, "description": "10å¹¶å‘å‹åŠ›æµ‹è¯•"},
        {"name": "æé™å¹¶å‘", "concurrent": 20, "total": 200, "description": "20å¹¶å‘æé™æµ‹è¯•"},
        
        # é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
        {"name": "æŒä¹…æ€§èƒ½", "concurrent": 8, "total": 160, "description": "æŒç»­è´Ÿè½½ç¨³å®šæ€§"},
        
        # çªå‘æµé‡æµ‹è¯•  
        {"name": "çªå‘æµé‡", "concurrent": 15, "total": 75, "description": "æ¨¡æ‹Ÿçªå‘è¯·æ±‚"}
    ]
    
    all_results = []
    
    for i, scenario in enumerate(performance_scenarios):
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æµ‹è¯• {i+1}/{len(performance_scenarios)}: {scenario['name']}")
        print(f"ğŸ“ è¯´æ˜: {scenario['description']}")
        print("="*80)
        
        # æ‰§è¡Œæµ‹è¯•åœºæ™¯
        result = run_performance_scenario(
            scenario['name'],
            scenario['concurrent'], 
            scenario['total'],
            test_prompt,
            api_url
        )
        
        if 'error' not in result:
            all_results.append(result)
        
        # æµ‹è¯•é—´éš”ï¼Œè®©é›†ç¾¤ç¨ä½œä¼‘æ¯
        if i < len(performance_scenarios) - 1:
            print(f"   â³ ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
            time.sleep(5)
    
    # æ€§èƒ½åˆ†ææ±‡æ€»
    print(f"\n{'='*80}")
    print("ğŸ“ˆ é›†ç¾¤æ€§èƒ½åˆ†ææ±‡æ€»")
    print("="*80)
    
    if all_results:
        # æå–å…³é”®æ€§èƒ½æŒ‡æ ‡
        throughput_data = [(r['test_config']['concurrent_requests'], r['results']['requests_per_second']) for r in all_results]
        latency_data = [(r['test_config']['concurrent_requests'], r['results']['avg_response_time']) for r in all_results]
        token_throughput_data = [(r['test_config']['concurrent_requests'], r['results']['tokens_per_second']) for r in all_results]
        
        print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        print(f"{'å¹¶å‘æ•°':<8} {'è¯·æ±‚ååé‡':<12} {'Tokenååé‡':<12} {'å¹³å‡å»¶è¿Ÿ':<10} {'P95å»¶è¿Ÿ':<10}")
        print("-" * 60)
        
        for result in all_results:
            config = result['test_config']
            metrics = result['results']
            print(f"{config['concurrent_requests']:<8} {metrics['requests_per_second']:<12.2f} "
                  f"{metrics['tokens_per_second']:<12.1f} {metrics['avg_response_time']:<10.3f} "
                  f"{metrics['p95_response_time']:<10.3f}")
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½ç‚¹
        max_throughput = max(all_results, key=lambda x: x['results']['requests_per_second'])
        max_token_throughput = max(all_results, key=lambda x: x['results']['tokens_per_second'])
        min_latency = min(all_results, key=lambda x: x['results']['avg_response_time'])
        
        print(f"\nğŸ† æ€§èƒ½å³°å€¼:")
        print(f"   â€¢ æœ€é«˜è¯·æ±‚ååé‡: {max_throughput['results']['requests_per_second']:.2f} req/s "
              f"(å¹¶å‘æ•°: {max_throughput['test_config']['concurrent_requests']})")
        print(f"   â€¢ æœ€é«˜Tokenååé‡: {max_token_throughput['results']['tokens_per_second']:.1f} tok/s "
              f"(å¹¶å‘æ•°: {max_token_throughput['test_config']['concurrent_requests']})")
        print(f"   â€¢ æœ€ä½å¹³å‡å»¶è¿Ÿ: {min_latency['results']['avg_response_time']:.3f}s "
              f"(å¹¶å‘æ•°: {min_latency['test_config']['concurrent_requests']})")
        
        # é›†ç¾¤æ•ˆç‡åˆ†æ
        avg_success_rate = statistics.mean([r['results']['success_rate'] for r in all_results])
        peak_tokens_per_second = max_token_throughput['results']['tokens_per_second']
        
        print(f"\nğŸ“ˆ é›†ç¾¤æ•´ä½“è¯„ä¼°:")
        print(f"   â€¢ å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1f}%")
        print(f"   â€¢ å³°å€¼Tokenå¤„ç†èƒ½åŠ›: {peak_tokens_per_second:.1f} tok/s")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        if peak_tokens_per_second > 500:
            performance_tier = "ğŸŒŸ å“è¶Š - é«˜æ€§èƒ½é›†ç¾¤"
        elif peak_tokens_per_second > 300:
            performance_tier = "âœ… ä¼˜ç§€ - ç”Ÿäº§çº§æ€§èƒ½"
        elif peak_tokens_per_second > 150:
            performance_tier = "âš ï¸  è‰¯å¥½ - åŸºæœ¬æ»¡è¶³éœ€æ±‚"
        else:
            performance_tier = "âŒ å¾…ä¼˜åŒ– - æ€§èƒ½ä¸è¶³"
        
        print(f"   â€¢ æ€§èƒ½ç­‰çº§: {performance_tier}")
        
        # å¹¶å‘æ‰©å±•æ€§åˆ†æ
        low_concurrent_tps = [r['results']['tokens_per_second'] for r in all_results if r['test_config']['concurrent_requests'] <= 5]
        high_concurrent_tps = [r['results']['tokens_per_second'] for r in all_results if r['test_config']['concurrent_requests'] >= 10]
        
        if low_concurrent_tps and high_concurrent_tps:
            scalability_ratio = max(high_concurrent_tps) / max(low_concurrent_tps)
            print(f"   â€¢ å¹¶å‘æ‰©å±•æ€§: {scalability_ratio:.2f}x (é«˜å¹¶å‘ç›¸å¯¹ä½å¹¶å‘çš„æ€§èƒ½æå‡)")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        summary_data = {
            "cluster_performance_evaluation": {
                "model": "Qwen/Qwen3-32B",
                "architecture": "PP=2, TP=8 (2 nodes, 16 GPUs)",
                "test_type": "é›†ç¾¤æ€§èƒ½ä¸“é¡¹æµ‹è¯•",
                "test_focus": "ååé‡ã€å»¶è¿Ÿã€å¹¶å‘å¤„ç†èƒ½åŠ›ç­‰ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡",
                "test_scenarios": all_results,
                "performance_summary": {
                    "max_request_throughput": max_throughput['results']['requests_per_second'],
                    "max_token_throughput": max_token_throughput['results']['tokens_per_second'], 
                    "min_avg_latency": min_latency['results']['avg_response_time'],
                    "avg_success_rate": avg_success_rate,
                    "performance_tier": performance_tier,
                    "optimal_concurrency": max_token_throughput['test_config']['concurrent_requests']
                },
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        with open("cluster_performance_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: cluster_performance_evaluation.json")
        print(f"ğŸ“„ è¯¥æ–‡ä»¶åŒ…å«å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡å’ŒåŸå§‹æ•°æ®")
        
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•æ€§èƒ½æµ‹è¯•")

if __name__ == "__main__":
    test_cluster_performance()