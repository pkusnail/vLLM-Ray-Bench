# 4-node large scale cluster configuration example
# Use case: 4 servers, each with 8 GPUs, totaling 32 GPUs

cluster:
  name: "qwen3-4node-cluster"
  max_concurrent_requests: 64

model:
  name: "Qwen/Qwen3-32B"
  max_model_len: 32768
  trust_remote_code: true
  gpu_memory_utilization: 0.75  # More conservative for larger clusters
  swap_space: 16
  quantization: null

distributed:
  strategy: "manual"  # Manual configuration for optimal performance
  tensor_parallel_size: 8  # TP=8 within each node
  pipeline_parallel_size: 4  # PP=4 across nodes

nodes:
  head:
    ip: "192.168.1.100"
    port: 6379
    dashboard_port: 8265
    gpus: 8
    cpus: 64  # Higher CPU count for head node
    memory: "512GB"
    
  workers:
    - ip: "192.168.1.101"
      gpus: 8
      cpus: 32
      memory: "320GB"
    - ip: "192.168.1.102"
      gpus: 8
      cpus: 32
      memory: "320GB"
    - ip: "192.168.1.103"
      gpus: 8
      cpus: 32
      memory: "320GB"

service:
  host: "0.0.0.0"
  port: 8000
  max_num_seqs: 32  # Higher for larger cluster
  
  autoscaling:
    min_replicas: 1
    max_replicas: 2  # Can scale to 2 replicas
    target_requests_per_replica: 16

network:
  nccl:
    use_default: true
    # For high-bandwidth networks, consider custom settings:
    # custom_env:
    #   NCCL_IB_DISABLE: "0"  # Enable InfiniBand if available
    #   NCCL_NET_GDR_LEVEL: "3"

environment:
  python_path: "./vllm_ray_env/bin/python"
  ray_env_path: "./vllm_ray_env"
  working_directory: "/home/ubuntu/vllm-ray-bench"

security:
  ssh_user: "ubuntu"
  ssh_key_path: "~/.ssh/id_rsa"
  api_key: "your-api-key-here"  # Optional API authentication

monitoring:
  enable_metrics: true
  prometheus_port: 9090
  log_level: "INFO"