# 4-node large scale cluster configuration template
# Use case: 4 servers, each with 8 GPUs, totaling 32 GPUs

cluster:
  name: "${CLUSTER_NAME}"
  max_concurrent_requests: 64

model:
  name: "${MODEL_NAME}"
  max_model_len: ${MAX_MODEL_LEN}
  trust_remote_code: true
  gpu_memory_utilization: ${GPU_MEMORY_UTILIZATION}
  swap_space: 16
  quantization: null

distributed:
  strategy: "manual"  # Manual configuration for optimal performance
  tensor_parallel_size: ${TENSOR_PARALLEL_SIZE}
  pipeline_parallel_size: ${PIPELINE_PARALLEL_SIZE}

nodes:
  head:
    ip: "${HEAD_NODE_IP}"
    port: 6379
    dashboard_port: ${RAY_DASHBOARD_PORT}
    gpus: 8
    cpus: 64  # Higher CPU count for head node
    memory: "512GB"
    
  workers:
    - ip: "${WORKER_NODE_IP_1}"
      gpus: 8
      cpus: 32
      memory: "320GB"
    - ip: "${WORKER_NODE_IP_2}"
      gpus: 8
      cpus: 32
      memory: "320GB"
    - ip: "${WORKER_NODE_IP_3}"
      gpus: 8
      cpus: 32
      memory: "320GB"

service:
  host: "0.0.0.0"
  port: ${VLLM_API_PORT}
  max_num_seqs: 32  # Higher for larger cluster
  
  autoscaling:
    min_replicas: 1
    max_replicas: 2  # Can scale to 2 replicas
    target_requests_per_replica: 16

network:
  nccl:
    use_default: true
    # For high-bandwidth networks, consider custom settings:
    # custom_env:
    #   NCCL_IB_DISABLE: "0"  # Enable InfiniBand if available
    #   NCCL_NET_GDR_LEVEL: "3"

environment:
  python_path: "./venv/bin/python"
  ray_env_path: "./venv"
  working_directory: "$(pwd)"

security:
  ssh_user: "ubuntu"
  ssh_key_path: "~/.ssh/id_rsa"
  api_key: "your-api-key-here"  # Optional API authentication

monitoring:
  enable_metrics: true
  prometheus_port: 9090
  log_level: "INFO"