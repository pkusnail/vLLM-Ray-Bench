# Evaluation Dependencies
# Packages for model benchmarking and evaluation

# Core Evaluation Frameworks
lm-eval[api]>=0.4.0
human-eval>=1.0.3
datasets>=2.14.0
evaluate>=0.4.0

# ModelScope Integration
modelscope>=1.9.0
ms-swift>=1.0.0

# Benchmark-specific Dependencies
scikit-learn>=1.3.0
rouge-score>=0.1.2
sacrebleu>=2.3.0
nltk>=3.8.0
absl-py>=1.4.0

# Utilities
fire>=0.5.0
termcolor>=2.3.0
colorama>=0.4.6
tabulate>=0.9.0
tqdm>=4.65.0
tenacity>=8.2.0

# Data Processing
jsonlines>=3.1.0
more-itertools>=9.1.0
word2number>=1.1

# Visualization (Optional)
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0